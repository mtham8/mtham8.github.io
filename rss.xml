<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[mic(s)]]></title><description><![CDATA[A place for thoughts to roam free.]]></description><link>https://mtham8.github.io/</link><generator>RSS for Node</generator><lastBuildDate>Sun, 17 May 2020 01:36:04 GMT</lastBuildDate><item><title><![CDATA[Vector Space Model and TF-IDF]]></title><description><![CDATA[There are many similarity models floating in the universe, holding different assumptions and priorities. Here we will explore the most…]]></description><link>https://mtham8.github.io//tf-idf/</link><guid isPermaLink="false">https://mtham8.github.io//tf-idf/</guid><pubDate>Sat, 16 May 2020 22:12:03 GMT</pubDate><content:encoded>&lt;p&gt;There are many similarity models floating in the universe, holding different assumptions and priorities. Here we will explore the most common one used, the TF-IDF Vector Space Model.&lt;/p&gt;
&lt;h2&gt;VSM (Vector Space Model)&lt;/h2&gt;
&lt;p&gt;A vector space model is a model where each term of the query is considered a vector dimension. It assumes that each term is a dimension that is orthogonal to all other terms, which means terms are modeled as occurring in documents independently. When evaluating the query to a document considered, one vector represents the query, and another, the document. The cosine similarity of the two vectors can be used to represent the relevance of the document to the query. A cosine value of 0 means that the query and the document vector are orthogonal and have no match (ie. none of the query terms were in the document).&lt;/p&gt;
&lt;p&gt;Let’s see how this can play out. We could set each coordinate in the document vector to be 1 if the document contains the term for the dimension, otherwise set it to 0. The query vector would be all 1’s, assuming each term has the same weight. This model would rank a document mentioning a query term once in a footnote, as equally relevant as, another document that uses a query term repeatedly throughout the text. Hmm… that ranking does not seem right. We should be able to calcuate different weights for a term in the document given its frequency and other factors that could affect its relevancy. Enter TF-IDF! A plausible way to account for various factors that this current model does not. Instead of using 1 if the term is present, perhaps, we can use TF-IDF in the document vector.&lt;/p&gt;
&lt;h2&gt;TF-IDF (Term Frequency-Inverse Document Frequency)&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Term Frequency-Inverse Document Frequency&lt;/em&gt;, is a numerical statistic that represents how influential a word is to defining the “relevancy” of a document in a corpus. The TF-IDF value increases proportionally to the frequency a word appears in a document (TF) and is offset by the number of documents in the corpus that also contain that word (IDF). The offset accounts for the fact that if a word, such as &lt;code class=&quot;language-text&quot;&gt;the&lt;/code&gt; or &lt;code class=&quot;language-text&quot;&gt;is&lt;/code&gt;, appears often in many documents, maybe it’s not as influential in defining the “uniqueness” or “relevancy” of a specific document. Another way of thinking about it is, for instance, if you have a corpus that is about cats, when you search &lt;code class=&quot;language-text&quot;&gt;can my cat eat chicken&lt;/code&gt;, perhaps &lt;code class=&quot;language-text&quot;&gt;cat&lt;/code&gt; is not as important as the other terms you used in your search, such as &lt;code class=&quot;language-text&quot;&gt;chicken&lt;/code&gt;, and documents with the word &lt;code class=&quot;language-text&quot;&gt;chicken&lt;/code&gt; should be more relevant to your search than having the word &lt;code class=&quot;language-text&quot;&gt;cat&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;TF-IDF is an intuitive concept, and works well under the assumption that each document in the corpus roughly has around the same length. However, if documents have varying lengths in the corpus, TF-IDF alone doesn’t not account for that. Perhaps we could add additional weights to this model to account for varying document lengths and how much influence it should have to the overall relevance value of a given document.&lt;/p&gt;
&lt;p&gt;In Lucene’s implementation of TF-IDF, &lt;code class=&quot;language-text&quot;&gt;ClassicSimilarity&lt;/code&gt;, an additional bias, &lt;code class=&quot;language-text&quot;&gt;lengthNorm&lt;/code&gt;, was introduced to account for document length. &lt;code class=&quot;language-text&quot;&gt;lengthNorm&lt;/code&gt; gave signficant bias towards matching shorter documents over longer documents. It assumed that if a term occurs the same amount of times in two documents, but one is shorter than the other, then the shorter one is more “concentrated” and therefore, scores higher in relevancy than the longer document. The implementation looked like this:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;IDF * TF * lengthNorm&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;IDF&lt;/code&gt; implemented as &lt;code class=&quot;language-text&quot;&gt;log((docCount+1)/(docFreq+1)) + 1&lt;/code&gt; where &lt;code class=&quot;language-text&quot;&gt;docFreq&lt;/code&gt; is number of documents containing the term, and &lt;code class=&quot;language-text&quot;&gt;docCount&lt;/code&gt; is total number of documents&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;TF&lt;/code&gt; implemented as &lt;code class=&quot;language-text&quot;&gt;sqrt(freq)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;lengthNorm&lt;/code&gt; implemented as &lt;code class=&quot;language-text&quot;&gt;1/sqrt(length)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Keep in mind, this implementation &lt;em&gt;accounts&lt;/em&gt; for document length, but does not allow you to adjust its &lt;em&gt;influence&lt;/em&gt; to the overall value.&lt;/p&gt;
&lt;h2&gt;Limitations&lt;/h2&gt;
&lt;p&gt;There are some limitations using the TF-IDF VSM. IDF is calculated not based on relevance information, but on matching terms. Documents with simliar context, but different term vocabulary would not be considered a match, therefore, resulting as false negatives. The positions of the words within a document is lost in the vector space representation. The model assumes the terms are independent. Under this assumption, documents are a &lt;em&gt;bag of words&lt;/em&gt; and order does not matter. Often that is not the case. Terms can be dependent in situations such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;polysemy&lt;/em&gt;: same terms used in different contexts - ie. &lt;code class=&quot;language-text&quot;&gt;I love my cat&lt;/code&gt; vs &lt;code class=&quot;language-text&quot;&gt;cat lady&lt;/code&gt;, which affects &lt;em&gt;precision&lt;/em&gt; because irrelevant documents can be retrieved due to term matching.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;synonymity&lt;/em&gt;: different terms use in the same contexts - ie. &lt;code class=&quot;language-text&quot;&gt;feline&lt;/code&gt; vs &lt;code class=&quot;language-text&quot;&gt;cat&lt;/code&gt;, which affects &lt;em&gt;recall&lt;/em&gt; because documents that are relevant would not be retrieved.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;ordering&lt;/em&gt;: different terms used in different positions in different contexts - ie. &lt;code class=&quot;language-text&quot;&gt;college junior&lt;/code&gt; vs &lt;code class=&quot;language-text&quot;&gt;junior college&lt;/code&gt;, which affects &lt;em&gt;precision&lt;/em&gt; and &lt;em&gt;recall&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nonetheless, TF-IDF VSM is popular because it can go pretty far! Every corpus has its flavor. I invite you to try TF-IDF VSM, with its limitations in mind so that the surprises left are the unknowns.&lt;/p&gt;
&lt;h2&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/search/similarities/ClassicSimilarity.java&quot;&gt;https://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/search/similarities/ClassicSimilarity.java&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://lucene.apache.org/core/7_4_0/core/org/apache/lucene/search/similarities/TFIDFSimilarity.html&quot;&gt;https://lucene.apache.org/core/7_4_0/core/org/apache/lucene/search/similarities/TFIDFSimilarity.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/the-binary-independence-model-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/the-binary-independence-model-1.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/&quot;&gt;https://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.elastic.co/blog/found-similarity-in-elasticsearch&quot;&gt;https://www.elastic.co/blog/found-similarity-in-elasticsearch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.minerazzi.com/tutorials/term-vector-3.pdf&quot;&gt;http://www.minerazzi.com/tutorials/term-vector-3.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Concurrency Bloopers]]></title><description><![CDATA[Earlier this week, I ran into a bug that I couldn’t quite understand. The code looked a bit like this https://play.golang.org/p/29mw6PEDm2t…]]></description><link>https://mtham8.github.io//concurrency-bloopers/</link><guid isPermaLink="false">https://mtham8.github.io//concurrency-bloopers/</guid><pubDate>Sun, 21 Jul 2019 22:12:03 GMT</pubDate><content:encoded>&lt;p&gt;Earlier this week, I ran into a bug that I couldn’t quite understand. The code looked a bit like this&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://play.golang.org/p/29mw6PEDm2t&quot;&gt;https://play.golang.org/p/29mw6PEDm2t&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;My task was to create a program that fetches data, processes it, and then loads the data into a database.
My initial thought was, okay, let’s think about this in terms of streams. A stream of incoming data flows in through
a channel - let’s call that channel, &lt;code class=&quot;language-text&quot;&gt;received&lt;/code&gt;. Some function will then read off of &lt;code class=&quot;language-text&quot;&gt;received&lt;/code&gt; and process each item in that channel, and then send the result to a stream of processed data, or to a channel called, &lt;code class=&quot;language-text&quot;&gt;processed&lt;/code&gt;. Some other function will then read off of &lt;code class=&quot;language-text&quot;&gt;processed&lt;/code&gt; and either load it to a database, or in the playground example, simply print out what was processed. Thinking in streams allowed me to decouple data structures from upstream and downstream processes.&lt;/p&gt;
&lt;p&gt;However, when I ran the program, I thought the results were quite weird. There was no errors, no race condition, but the only channel being read was the &lt;code class=&quot;language-text&quot;&gt;received&lt;/code&gt; channel. Then the &lt;code class=&quot;language-text&quot;&gt;processed&lt;/code&gt; channel was read afterwards. Hm. That was not what I was expecting. The code that ran behaved synchronously, despite the fact that the mental model in my mind, was concurrent.
I was expecting one datum to be sent to the &lt;code class=&quot;language-text&quot;&gt;received&lt;/code&gt; channel, and then consequently, in order for the &lt;code class=&quot;language-text&quot;&gt;received&lt;/code&gt; channel to send another datum, &lt;code class=&quot;language-text&quot;&gt;processed&lt;/code&gt; channel would be read.&lt;/p&gt;
&lt;p&gt;After a couple hours of debugging, I saw one minor but enormous bug. Here is the correction, based on the example above&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://play.golang.org/p/PUx_TXX6esV&quot;&gt;https://play.golang.org/p/PUx_TXX6esV&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I was reading off the &lt;code class=&quot;language-text&quot;&gt;received&lt;/code&gt; channel on the same thread as reading off the &lt;code class=&quot;language-text&quot;&gt;processed&lt;/code&gt; channel. Therefore, &lt;code class=&quot;language-text&quot;&gt;processed&lt;/code&gt; could not be read until &lt;code class=&quot;language-text&quot;&gt;received&lt;/code&gt; was closed. The solution was to wrap reading off the &lt;code class=&quot;language-text&quot;&gt;received&lt;/code&gt; channel in a goroutine so that each channel can be read in its own thread/goroutine.&lt;/p&gt;
&lt;p&gt;Concurrency is complex. Always make sure to verify if the code written reflects the mental model correctly. If you’re going to return a channel, wrap the process that sends to that channel in a goroutine.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Let's Merge Channels]]></title><description><![CDATA[I encountered a situation while writing an application for text extraction,
where I fan-out to process files based on file extension types…]]></description><link>https://mtham8.github.io//merge-channels/</link><guid isPermaLink="false">https://mtham8.github.io//merge-channels/</guid><pubDate>Sat, 06 Jul 2019 22:12:03 GMT</pubDate><content:encoded>&lt;p&gt;I encountered a situation while writing an application for text extraction,
where I fan-out to process files based on file extension types, but then needed a
way to fan-in the results to a single channel of responses for some consumer.&lt;/p&gt;
&lt;p&gt;After doing some open-source research (googling), I discovered go had a convenient way
of merging channels, which in this case, was processed data channels produced by each file extension type (struct)
that implemented the file processor interface.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;go&quot;&gt;&lt;pre class=&quot;language-go&quot;&gt;&lt;code class=&quot;language-go&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;mergeChannels&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;cs &lt;span class=&quot;token operator&quot;&gt;...&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;token keyword&quot;&gt;chan&lt;/span&gt; ProcessorInterface&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;token keyword&quot;&gt;chan&lt;/span&gt; ProcessorInterface &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
	outChan &lt;span class=&quot;token operator&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token keyword&quot;&gt;chan&lt;/span&gt; ProcessorInterface&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;token keyword&quot;&gt;var&lt;/span&gt; wg sync&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;WaitGroup
	wg&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;Add&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;cs&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

	&lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; c &lt;span class=&quot;token operator&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;range&lt;/span&gt; cs &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;token keyword&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;channel &lt;span class=&quot;token operator&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;token keyword&quot;&gt;chan&lt;/span&gt; ProcessorInterface&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
			&lt;span class=&quot;token keyword&quot;&gt;defer&lt;/span&gt; wg&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;Done&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
			&lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; val &lt;span class=&quot;token operator&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;range&lt;/span&gt; channel &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
				outChan &lt;span class=&quot;token operator&quot;&gt;&amp;lt;-&lt;/span&gt; val
			&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;
		&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;c&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

	&lt;span class=&quot;token keyword&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
		wg&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;Wait&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;token function&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;outChan&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; outChan
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content:encoded></item></channel></rss>