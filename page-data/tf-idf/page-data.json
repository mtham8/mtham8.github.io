{"componentChunkName":"component---src-templates-blog-post-js","path":"/tf-idf/","webpackCompilationHash":"91a9e76ae362503cd6e8","result":{"data":{"site":{"siteMetadata":{"title":"mic(s)","author":"Michelle"}},"markdownRemark":{"id":"056eaeb1-c52f-50ed-bb6a-ab219c8e88b0","excerpt":"There are many similarity models floating in the universe, holding different assumptions and priorities. Here we will explore the most common one used, the TF‚Ä¶","html":"<p>There are many similarity models floating in the universe, holding different assumptions and priorities. Here we will explore the most common one used, the TF-IDF Vector Space Model.</p>\n<h2>VSM (Vector Space Model)</h2>\n<p>A vector space model is a model where each term of the query is considered a vector dimension. It assumes that each term is a dimension that is orthogonal to all other terms, which means terms are modeled as occurring in documents independently. When evaluating the query to a document considered, one vector represents the query, and another, the document. The cosine similarity of the two vectors can be used to represent the relevance of the document to the query. A cosine value of 0 means that the query and the document vector are orthogonal and have no match (ie. none of the query terms were in the document). Cosine similarity is advantageous over Euclidean distance because cosine similarity measures the angle between two vectors, which means it captures the <em>direction</em> of the two vectors - while Euclidean distance captures the <em>magnitude</em>. Two vectors can be far apart by Euclidean distance, ie. imagine a short vector and a long vector, but have a small angle between them.</p>\n<p>Let‚Äôs see how this can play out. We could set each coordinate in the document vector to be 1 if the document contains the term for the dimension, otherwise set it to 0. The query vector would be all 1‚Äôs, assuming each term has the same weight. Here‚Äôs an example:</p>\n<ul>\n<li>\n<p>query: <code class=\"language-text\">can my cat eat chicken</code></p>\n<ul>\n<li>vector: [1, 1, 1, 1, 1]</li>\n</ul>\n</li>\n<li>\n<p>document1: <code class=\"language-text\">A cat can eat chicken. Chicken is part of a cat&#39;s diet.</code></p>\n<ul>\n<li>vector: [1, 0, 1, 1, 1]</li>\n<li>score: 4</li>\n</ul>\n</li>\n<li>\n<p>document2: <code class=\"language-text\">The cat ran past the chicken to try to eat my mouse.</code></p>\n<ul>\n<li>vector: [0, 1, 1, 1, 1]</li>\n<li>score: 4</li>\n</ul>\n</li>\n</ul>\n<p>This model would rank <code class=\"language-text\">document1</code> and <code class=\"language-text\">document2</code> equally relevant to <code class=\"language-text\">query</code>. It rank a document mentioning a query term once in a footnote, as relevant as, another document that uses a query term repeatedly throughout the text. Hmm‚Ä¶ the rankings do not seem right. We should be able to calcuate different weights for a term in the document given its frequency and other factors that could affect its relevancy. Enter TF-IDF! A plausible way to account for various factors that this current model does not. Instead of using 1 if the term is present, perhaps, we can use TF-IDF in the document vector.</p>\n<h2>TF-IDF (Term Frequency-Inverse Document Frequency)</h2>\n<p><em>Term Frequency-Inverse Document Frequency</em>, is a numerical statistic that represents how influential a word is to defining the ‚Äúrelevancy‚Äù of a document in a corpus. The TF-IDF value increases proportionally to the frequency a word appears in a document (TF) and is offset by the number of documents in the corpus that also contain that word (IDF). The offset accounts for the fact that if a word, such as <code class=\"language-text\">the</code> or <code class=\"language-text\">is</code>, appears often in many documents, maybe it‚Äôs not as influential in defining the ‚Äúuniqueness‚Äù or ‚Äúrelevancy‚Äù of a specific document. Another way of thinking about it is, for instance, if you have a corpus that is about cats, when you search <code class=\"language-text\">can my cat eat chicken</code>, perhaps <code class=\"language-text\">cat</code> is not as important as the other terms you used in your search, such as <code class=\"language-text\">chicken</code>, and documents with the word <code class=\"language-text\">chicken</code> should be more relevant to your search than having the word <code class=\"language-text\">cat</code>.</p>\n<p>TF-IDF is an intuitive concept, and works well under the assumption that each document in the corpus roughly has around the same length. However, if documents have varying lengths in the corpus, TF-IDF alone doesn‚Äôt not account for that. Perhaps we could add additional weights to this model to account for varying document lengths and how much influence it should have to the overall relevance value of a given document.</p>\n<p>In Lucene‚Äôs implementation of TF-IDF, <code class=\"language-text\">ClassicSimilarity</code>, an additional bias, <code class=\"language-text\">lengthNorm</code>, was introduced to account for document length. <code class=\"language-text\">lengthNorm</code> gave signficant bias towards matching shorter documents over longer documents. It assumed that if a term occurs the same amount of times in two documents, but one is shorter than the other, then the shorter one is more ‚Äúconcentrated‚Äù and therefore, scores higher in relevancy than the longer document. The implementation looked like this:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">IDF * TF * lengthNorm</code></pre></div>\n<ul>\n<li><code class=\"language-text\">IDF</code> implemented as <code class=\"language-text\">log((docCount+1)/(docFreq+1)) + 1</code> where <code class=\"language-text\">docFreq</code> is number of documents containing the term, and <code class=\"language-text\">docCount</code> is total number of documents</li>\n<li><code class=\"language-text\">TF</code> implemented as <code class=\"language-text\">sqrt(freq)</code></li>\n<li><code class=\"language-text\">lengthNorm</code> implemented as <code class=\"language-text\">1/sqrt(length)</code></li>\n</ul>\n<p>Keep in mind, this implementation <em>accounts</em> for document length, but does not allow you to adjust its <em>influence</em> to the overall value.</p>\n<p>Let‚Äôs see how using TF-IDF weighting, based on Lucene‚Äôs implementation, changes the relevancy scores in our earlier example.</p>\n<ul>\n<li>query: <code class=\"language-text\">can my cat eat chicken</code></li>\n<li>\n<p>document1: <code class=\"language-text\">A cat can eat chicken. Chicken is part of a cat&#39;s diet.</code></p>\n<ul>\n<li>vector: [0.2886751345948129, 0, 0.1716274399381882, 0.1716274399381882, 0.24271785323595957]</li>\n<li>score: 0.8746478677071488</li>\n</ul>\n</li>\n<li>\n<p>document2: <code class=\"language-text\">The cat ran past the chicken to try to eat my mouse.</code></p>\n<ul>\n<li>vector: [0, 0.2886751345948129, 0.1716274399381882, 0.1716274399381882, 0.1716274399381882]</li>\n<li>score: 0.8035574544093774</li>\n</ul>\n</li>\n</ul>\n<p>TF-IDF Implementation: <a href=\"https://gist.github.com/mtham8/8f93db2443b6214dc0734c01d029b80c\">https://gist.github.com/mtham8/8f93db2443b6214dc0734c01d029b80c</a></p>\n<p>Playground: <a href=\"https://play.golang.org/p/XZOC8fHUyWs\">https://play.golang.org/p/XZOC8fHUyWs</a></p>\n<p>We can see here <code class=\"language-text\">document1</code> is ranked higher than <code class=\"language-text\">document2</code> in relevancy to <code class=\"language-text\">query</code> using TF-IDF weighting. Hooray! This ranking is closer to what we are looking for. There were a few text cleaning steps I had to do beforehand, ie. lowercasing all the terms, and omitting some punctuations (periods). I also tokenized the documents, ie. splitting the sentences into terms. These transformations are common preprocessing steps, and depending on the corpus and search needs, one might preprocess the text differently.</p>\n<h2>Limitations</h2>\n<p>There are some limitations using the TF-IDF VSM. IDF is calculated not based on relevance information, but on matching terms. Documents with simliar context, but different term vocabulary would not be considered a match, therefore, resulting in false negatives. The positions of the words within a document is lost in the vector space representation. The model assumes the terms are independent. Under this assumption, documents are a <em>bag of words</em> and order does not matter. Often that is not the case. Terms can be dependent in situations such as:</p>\n<ul>\n<li><em>polysemy</em>: same terms used in different contexts - ie. <code class=\"language-text\">I love my cat</code> vs <code class=\"language-text\">cat lady</code>, which affects <em>precision</em> because irrelevant documents can be retrieved due to term matching.</li>\n<li><em>synonymity</em>: different terms use in the same contexts - ie. <code class=\"language-text\">feline</code> vs <code class=\"language-text\">cat</code>, which affects <em>recall</em> because documents that are relevant would not be retrieved.</li>\n<li><em>ordering</em>: different terms used in different positions in different contexts - ie. <code class=\"language-text\">college junior</code> vs <code class=\"language-text\">junior college</code>, which affects <em>precision</em> and <em>recall</em>.</li>\n</ul>\n<p>Nonetheless, TF-IDF VSM is popular because it can go pretty far! Every corpus has its flavor. I invite you to try TF-IDF VSM, with its limitations in mind so that the surprises left are the unknowns. üöÄ</p>\n<h2>Sources</h2>\n<ul>\n<li><a href=\"https://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/search/similarities/ClassicSimilarity.java\">https://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/search/similarities/ClassicSimilarity.java</a></li>\n<li><a href=\"https://lucene.apache.org/core/7_4_0/core/org/apache/lucene/search/similarities/TFIDFSimilarity.html\">https://lucene.apache.org/core/7_4_0/core/org/apache/lucene/search/similarities/TFIDFSimilarity.html</a></li>\n<li><a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/the-binary-independence-model-1.html\">https://nlp.stanford.edu/IR-book/html/htmledition/the-binary-independence-model-1.html</a></li>\n<li><a href=\"https://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/\">https://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/</a></li>\n<li><a href=\"https://www.elastic.co/blog/found-similarity-in-elasticsearch\">https://www.elastic.co/blog/found-similarity-in-elasticsearch</a></li>\n<li><a href=\"http://www.minerazzi.com/tutorials/term-vector-3.pdf\">http://www.minerazzi.com/tutorials/term-vector-3.pdf</a></li>\n</ul>","frontmatter":{"title":"Vector Space Model and TF-IDF","date":"May 16, 2020","description":"There are many similarity models floating in the universe..."}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/tf-idf/","previous":{"fields":{"slug":"/concurrency-bloopers/"},"frontmatter":{"title":"Concurrency Bloopers"}},"next":{"fields":{"slug":"/BM25/"},"frontmatter":{"title":""}}}}}